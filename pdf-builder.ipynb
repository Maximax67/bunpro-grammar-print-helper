{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF Builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from PyPDF2 import PdfReader, PdfWriter, PageObject\n",
    "from reportlab.lib.pagesizes import A4\n",
    "from reportlab.pdfbase import pdfmetrics\n",
    "from reportlab.pdfbase.ttfonts import TTFont\n",
    "from reportlab.pdfgen import canvas\n",
    "from io import BytesIO\n",
    "\n",
    "\n",
    "pdfmetrics.registerFont(\n",
    "    TTFont(\"NotoSansJP-Regular\", \"./fonts/NotoSansJP-Regular.ttf\")\n",
    ")\n",
    "pdfmetrics.registerFont(\n",
    "    TTFont(\"NotoSansJP-Bold\", \"./fonts/NotoSansJP-Bold.ttf\")\n",
    ")\n",
    "\n",
    "pdf_dir = \"./pdf\"\n",
    "\n",
    "with open(\"parsed_data.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_text_page(\n",
    "    text: str,\n",
    "    description: str = None,\n",
    "    page_size: tuple[float, float] = A4,\n",
    ") -> PageObject:\n",
    "    packet = BytesIO()\n",
    "    c = canvas.Canvas(packet, pagesize=page_size)\n",
    "    width, height = page_size\n",
    "\n",
    "    c.setFillColorRGB(8/255, 38/255, 48/255)\n",
    "    c.rect(0, 0, width, height, fill=True)\n",
    "\n",
    "    c.setFillColorRGB(1, 1, 1)\n",
    "    c.setFont(\"NotoSansJP-Bold\", 36)\n",
    "    c.drawCentredString(width / 2, 2 * height / 3, text)\n",
    "\n",
    "    if description:\n",
    "        c.setFont(\"NotoSansJP-Regular\", 16)\n",
    "        c.drawCentredString(width / 2, 2 * height / 3 - 36 * 1.5, description)\n",
    "\n",
    "    c.save()\n",
    "    packet.seek(0)\n",
    "\n",
    "    text_pdf = PdfReader(packet)\n",
    "\n",
    "    return text_pdf.pages[0]\n",
    "\n",
    "\n",
    "pdf_writer = PdfWriter()\n",
    "pdf_reader = PdfReader(\"./title-page.pdf\")\n",
    "pdf_writer.add_page(pdf_reader.pages[0])\n",
    "\n",
    "pdf_reader = PdfReader(\"./structure.pdf\")\n",
    "structure_page = pdf_reader.pages[0]\n",
    "\n",
    "pdf_writer.add_page(structure_page)\n",
    "pdf_writer.add_outline_item(\"Structure\", 1)\n",
    "\n",
    "page_counter = 2\n",
    "bookmarks = []\n",
    "item_page_by_url: dict[str, int] = {}\n",
    "\n",
    "decks_to_skip_lessons_pages = {\"Non-JLPT\"}\n",
    "\n",
    "for entry in data[\"data\"]:\n",
    "    deck_name = entry[\"deck\"]\n",
    "    deck_page = create_text_page(f\"{deck_name} Deck\")\n",
    "    pdf_writer.add_page(deck_page)\n",
    "    deck_outline = pdf_writer.add_outline_item(f\"{deck_name} Deck\", page_counter)\n",
    "    skip_lesson_pages = deck_name in decks_to_skip_lessons_pages\n",
    "    page_counter += 1\n",
    "\n",
    "    for lesson in entry[\"lessons\"]:\n",
    "        title = lesson[\"title\"]\n",
    "\n",
    "        if not skip_lesson_pages:\n",
    "            lesson_number = lesson[\"lesson_number\"]\n",
    "            lesson_page = create_text_page(f\"Lesson {lesson_number}\", title)\n",
    "            pdf_writer.add_page(lesson_page)\n",
    "            lesson_outline = pdf_writer.add_outline_item(\n",
    "                f\"{lesson_number}. {title}\" if title else str(lesson_number),\n",
    "                page_counter,\n",
    "                deck_outline,\n",
    "            )\n",
    "\n",
    "        page_counter += 1\n",
    "\n",
    "        for item in lesson[\"items\"]:\n",
    "            pdf_filename = item[\"filename\"]\n",
    "            pdf_filepath = os.path.join(pdf_dir, pdf_filename)\n",
    "\n",
    "            item_name = item[\"name\"]\n",
    "            description = item[\"description\"]\n",
    "            item_url = item[\"url\"]\n",
    "            item_page_by_url[item_url] = page_counter\n",
    "\n",
    "            if os.path.exists(pdf_filepath):\n",
    "                pdf_reader = PdfReader(pdf_filepath)\n",
    "\n",
    "                for page in pdf_reader.pages:\n",
    "                    pdf_writer.add_page(page)\n",
    "\n",
    "                pdf_writer.add_outline_item(\n",
    "                    f\"{item_name}: {description}\", page_counter, deck_outline if skip_lesson_pages else lesson_outline\n",
    "                )\n",
    "\n",
    "                page_counter += len(pdf_reader.pages)\n",
    "            else:\n",
    "                print(f\"Pdf file {pdf_filename} not found for item: {item_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_filename = \"combined.pdf\"\n",
    "\n",
    "with open(output_filename, \"wb\") as output_pdf_file:\n",
    "    pdf_writer.write(output_pdf_file)\n",
    "\n",
    "print(f\"PDF saved as {output_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "del pdf_writer, pdf_reader, structure_page, page\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace bunpro links to pdf internal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def parse_grammar_point_ids(url: str) -> dict[str, str] | None:\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        data = {}\n",
    "        items = soup.find_all(\"li\", class_=\"search-tile_index\")\n",
    "\n",
    "        for item in items:\n",
    "            item_id = item.get(\"id\", \"\")\n",
    "            if item_id.startswith(\"grammar-point-id-\"):\n",
    "                item_id = item_id.replace(\"grammar-point-id-\", \"\")\n",
    "\n",
    "            link_tag = item.find(\"a\", href=True)\n",
    "            if link_tag:\n",
    "                href = link_tag[\"href\"]\n",
    "                data[item_id] = href[href.rfind(\"/\") + 1 :]\n",
    "            else:\n",
    "                print(f\"Missing url for item with id: {item_id}\")\n",
    "\n",
    "        return data\n",
    "    else:\n",
    "        print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "\n",
    "grammar_points_ids = parse_grammar_point_ids(\"https://bunpro.jp/grammar_points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz # PyMuPDF\n",
    "\n",
    "output_filename_2 = \"replaced_links.pdf\"\n",
    "pdf_document = fitz.open(output_filename)\n",
    "\n",
    "for page_num in range(pdf_document.page_count):\n",
    "    page = pdf_document[page_num]\n",
    "    links = page.get_links()\n",
    "\n",
    "    for link in links:\n",
    "        uri = link.get(\"uri\")\n",
    "        if uri and uri.startswith(\"https://bunpro.jp/grammar_points/\"):\n",
    "            target_page = item_page_by_url.get(uri)\n",
    "\n",
    "            # Replace '[' and ']' with %5B and %5D\n",
    "            if not target_page and ('[' in uri or ']' in uri):\n",
    "                modified_uri = uri.replace('[', '%5B').replace(']', '%5D')\n",
    "                target_page = item_page_by_url.get(modified_uri)\n",
    "\n",
    "            if not target_page:\n",
    "                item_path = uri[uri.rfind(\"/\") + 1 :]\n",
    "                item_path_from_id = grammar_points_ids.get(item_path)\n",
    "                if item_path_from_id:\n",
    "                    uri = \"https://bunpro.jp/grammar_points/\" + item_path_from_id\n",
    "                    target_page = item_page_by_url.get(uri)\n",
    "\n",
    "            if target_page:\n",
    "                if target_page != page_num:\n",
    "                    page.delete_link(link)\n",
    "                    page.insert_link(\n",
    "                        {\n",
    "                            \"kind\": fitz.LINK_GOTO,\n",
    "                            \"page\": target_page,\n",
    "                            \"from\": link[\"from\"],\n",
    "                        }\n",
    "                    )\n",
    "            else:\n",
    "                print(f\"Can not resolve link to item: {uri}\")\n",
    "\n",
    "pdf_document.save(output_filename_2)\n",
    "pdf_document.close()\n",
    "\n",
    "print(f\"PDF saved as {output_filename_2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linearize a PDF to enable faster loading (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pikepdf\n",
    "\n",
    "output_filename_3 = \"Bunpro-Grammar-Book.pdf\"\n",
    "\n",
    "with pikepdf.open(output_filename_2) as pdf:\n",
    "    pdf.save(output_filename_3, linearize=True)\n",
    "\n",
    "print(f\"PDF saved as {output_filename_3}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
